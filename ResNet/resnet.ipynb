{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "################### This is a line of 70 characters ##################\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils import write_file_and_close, check_control\n",
    "\n",
    "import os\n",
    "import errno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_batch_size = 128\n",
    "global_resnet_n = 3\n",
    "global_conv_bias = True\n",
    "global_data_print_freq = 20\n",
    "global_epoch_num = 200\n",
    "global_cuda_available = True\n",
    "global_output_filename = \"out.txt\"\n",
    "global_control_filename = \"control.txt\"\n",
    "global_epoch_test_freq = 1\n",
    "\n",
    "if global_cuda_available:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Here per-pixel mean isn't subtracted \n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", download=True, train=True, transform=transform_train\n",
    ")\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=global_batch_size, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", download=True, train=False, transform=transform_test\n",
    ")\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=global_batch_size, shuffle=False, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StartBlock(nn.Module):\n",
    "    \"\"\"First several blocks for resnet\n",
    "    \n",
    "    Only contains a single layer of conv2d and a batch norm layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, out_planes, kernel_size):\n",
    "        super(StartBlock, self).__init__()\n",
    "        self.out_plane = out_planes\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        self.conv = nn.Conv2d(\n",
    "            3, out_planes, kernel_size=kernel_size,\n",
    "            padding=1, bias=global_conv_bias\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_planes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = functional.relu(out)\n",
    "        return out\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"Repeated blockes for resnet\n",
    "    \n",
    "    Contains two conv layers, two batch norm layers and a shortcut\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.in_planes = in_planes\n",
    "        self.out_planes = out_planes\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, out_planes, kernel_size=kernel_size,\n",
    "            stride=stride, padding=1, bias=global_batch_size\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_planes, out_planes, kernel_size=kernel_size,\n",
    "            padding=1, bias=global_batch_size\n",
    "        )\n",
    "        self.shortcut = nn.Conv2d(\n",
    "            in_planes, out_planes, kernel_size=1, stride=stride\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.bn1(x)\n",
    "        out = functional.relu(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn2(out)\n",
    "        out = functional.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out += self.shortcut(x)\n",
    "        return out\n",
    "\n",
    "class EndBlock(nn.Module):\n",
    "    \"\"\"Last several blocks for resnet\n",
    "    \n",
    "    Only contains a global average pooling layer and a fully\n",
    "    connected layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_planes):\n",
    "        super(EndBlock, self).__init__()\n",
    "        self.fc = nn.Linear(in_planes, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.mean(x, dim=2)\n",
    "        out = torch.mean(out, dim=3)\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    \"\"\"ResNet-(6n + 2)\"\"\"\n",
    "    \n",
    "    def __init__(self, n):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.block_list = []\n",
    "        self.block_list.append(StartBlock(16, 3))\n",
    "        for i in range(n):\n",
    "            self.block_list.append(BasicBlock(16, 16, 3, 1))\n",
    "        self.block_list.append(BasicBlock(16, 32, 3, 2))\n",
    "        for i in range(n - 1):\n",
    "            self.block_list.append(BasicBlock(32, 32, 3, 1))\n",
    "        self.block_list.append(BasicBlock(32, 64, 3, 2))\n",
    "        for i in range(n - 1):\n",
    "            self.block_list.append(BasicBlock(64, 64, 3, 1))\n",
    "        self.block_list.append(EndBlock(64))\n",
    "        self.blocks = nn.Sequential(*self.block_list)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.blocks(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = ResNet(global_resnet_n)\n",
    "\n",
    "if global_cuda_available:\n",
    "    net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(\n",
    "    net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0001\n",
    ")\n",
    "\n",
    "def lr_adjust(it):\n",
    "    if it < 32000:\n",
    "        return 0.1\n",
    "    elif it < 48000:\n",
    "        return 0.01\n",
    "    elif it < 64000:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(data, info):\n",
    "    global net, optimizer, criterion\n",
    "    inputs, labels = data\n",
    "    inputs, labels = Variable(inputs), Variable(labels)\n",
    "    if global_cuda_available:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    info[0] = loss.data[0]\n",
    "    info[1] = labels.size()[0]\n",
    "\n",
    "def test(info):\n",
    "    global net\n",
    "    correct_sum = 0\n",
    "    total_loss_sum = 0.\n",
    "    total_ctr = 0\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        if global_cuda_available:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_ctr += labels.size()[0]\n",
    "        correct_sum += (predicted == labels.data).sum()\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss_sum += loss.data[0]\n",
    "    info[0] = correct_sum\n",
    "    info[1] = total_ctr\n",
    "    info[2] = total_loss_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning...\n",
      "The length of trainloader and testloader is 391 and 79 resp.\n"
     ]
    }
   ],
   "source": [
    "write_file_and_close(global_output_filename, \"Cleaning...\", flag = \"w\")\n",
    "write_file_and_close(\n",
    "    global_output_filename,\n",
    "    \"The length of trainloader and testloader is {:d} and {:d} resp.\"\n",
    "    .format(len(trainloader), len(testloader))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "epoch: 0, train set index: 19, average loss: 2.2259675264\n",
      "epoch: 0, train set index: 39, average loss: 2.0835567236\n",
      "epoch: 0, train set index: 59, average loss: 1.9823382616\n",
      "epoch: 0, train set index: 79, average loss: 1.9395702779\n",
      "epoch: 0, train set index: 99, average loss: 1.8971093476\n",
      "epoch: 0, train set index: 119, average loss: 1.8568778574\n",
      "epoch: 0, train set index: 139, average loss: 1.8022962272\n",
      "epoch: 0, train set index: 159, average loss: 1.7724299133\n",
      "epoch: 0, train set index: 179, average loss: 1.7696665227\n",
      "epoch: 0, train set index: 199, average loss: 1.7922856271\n",
      "epoch: 0, train set index: 219, average loss: 1.7823725343\n",
      "epoch: 0, train set index: 239, average loss: 1.7319254100\n",
      "epoch: 0, train set index: 259, average loss: 1.7484971225\n",
      "epoch: 0, train set index: 279, average loss: 1.7026990592\n",
      "epoch: 0, train set index: 299, average loss: 1.6513361394\n",
      "epoch: 0, train set index: 319, average loss: 1.6307328820\n",
      "epoch: 0, train set index: 339, average loss: 1.5940909266\n",
      "epoch: 0, train set index: 359, average loss: 1.6064390481\n",
      "epoch: 0, train set index: 379, average loss: 1.5287316024\n",
      "Epoch 0 finished, loss: 0.0139946954\n",
      "Starting testing\n",
      "Correct: 4341, total: 10000, accuracy: 0.4341000000, average loss: 0.0121869056\n",
      "Finished testing\n",
      "epoch: 1, train set index: 19, average loss: 1.5251911283\n",
      "epoch: 1, train set index: 39, average loss: 1.5300988913\n",
      "epoch: 1, train set index: 59, average loss: 1.4847040474\n",
      "epoch: 1, train set index: 79, average loss: 1.4751650512\n",
      "epoch: 1, train set index: 99, average loss: 1.4542636573\n",
      "epoch: 1, train set index: 119, average loss: 1.4332566917\n",
      "epoch: 1, train set index: 139, average loss: 1.3806114197\n",
      "epoch: 1, train set index: 159, average loss: 1.4623405695\n",
      "epoch: 1, train set index: 179, average loss: 1.4553666770\n",
      "epoch: 1, train set index: 199, average loss: 1.3595588565\n",
      "epoch: 1, train set index: 219, average loss: 1.3346527517\n",
      "epoch: 1, train set index: 239, average loss: 1.3210304976\n",
      "epoch: 1, train set index: 259, average loss: 1.3081962943\n",
      "epoch: 1, train set index: 279, average loss: 1.3022923172\n",
      "epoch: 1, train set index: 299, average loss: 1.2834659457\n",
      "epoch: 1, train set index: 319, average loss: 1.2632982016\n",
      "epoch: 1, train set index: 339, average loss: 1.3013717890\n",
      "epoch: 1, train set index: 359, average loss: 1.2437110305\n",
      "epoch: 1, train set index: 379, average loss: 1.2875128806\n",
      "Epoch 1 finished, loss: 0.0107466144\n",
      "Starting testing\n",
      "Correct: 5454, total: 10000, accuracy: 0.5454000000, average loss: 0.0100569536\n",
      "Finished testing\n",
      "epoch: 2, train set index: 19, average loss: 1.2009252787\n",
      "epoch: 2, train set index: 39, average loss: 1.1481880009\n",
      "epoch: 2, train set index: 59, average loss: 1.1660483718\n",
      "epoch: 2, train set index: 79, average loss: 1.2099689066\n",
      "epoch: 2, train set index: 99, average loss: 1.1910368085\n",
      "epoch: 2, train set index: 119, average loss: 1.1356355876\n",
      "epoch: 2, train set index: 139, average loss: 1.0918636531\n",
      "epoch: 2, train set index: 159, average loss: 1.1257085264\n",
      "epoch: 2, train set index: 179, average loss: 1.1126927257\n",
      "epoch: 2, train set index: 199, average loss: 1.1028594583\n",
      "epoch: 2, train set index: 219, average loss: 1.0963713646\n",
      "epoch: 2, train set index: 239, average loss: 1.0943269908\n",
      "epoch: 2, train set index: 259, average loss: 1.1075017601\n",
      "epoch: 2, train set index: 279, average loss: 1.1324143350\n",
      "epoch: 2, train set index: 299, average loss: 1.0688861489\n",
      "epoch: 2, train set index: 319, average loss: 1.0259934723\n",
      "epoch: 2, train set index: 339, average loss: 1.0691705287\n",
      "epoch: 2, train set index: 359, average loss: 1.0520175606\n",
      "epoch: 2, train set index: 379, average loss: 1.0956916749\n",
      "Epoch 2 finished, loss: 0.0087115274\n",
      "Starting testing\n",
      "Correct: 6184, total: 10000, accuracy: 0.6184000000, average loss: 0.0083022667\n",
      "Finished testing\n",
      "epoch: 3, train set index: 19, average loss: 1.0122997373\n",
      "epoch: 3, train set index: 39, average loss: 1.0433742374\n",
      "epoch: 3, train set index: 59, average loss: 1.0157698065\n",
      "epoch: 3, train set index: 79, average loss: 0.9660951912\n",
      "epoch: 3, train set index: 99, average loss: 0.9571671218\n",
      "epoch: 3, train set index: 119, average loss: 1.0253278822\n",
      "epoch: 3, train set index: 139, average loss: 0.9694577396\n",
      "epoch: 3, train set index: 159, average loss: 0.9636860132\n",
      "epoch: 3, train set index: 179, average loss: 0.9975393057\n",
      "epoch: 3, train set index: 199, average loss: 0.9597773939\n",
      "epoch: 3, train set index: 219, average loss: 0.9931239218\n",
      "epoch: 3, train set index: 239, average loss: 0.9973909348\n",
      "epoch: 3, train set index: 259, average loss: 0.9115142137\n",
      "epoch: 3, train set index: 279, average loss: 0.9249176115\n",
      "epoch: 3, train set index: 299, average loss: 0.9779628575\n",
      "epoch: 3, train set index: 319, average loss: 0.9337458432\n",
      "epoch: 3, train set index: 339, average loss: 0.9289842725\n",
      "epoch: 3, train set index: 359, average loss: 0.9606534421\n",
      "epoch: 3, train set index: 379, average loss: 0.9016649038\n",
      "Epoch 3 finished, loss: 0.0075658813\n",
      "Starting testing\n",
      "Correct: 6446, total: 10000, accuracy: 0.6446000000, average loss: 0.0081447468\n",
      "Finished testing\n",
      "epoch: 4, train set index: 19, average loss: 0.8656084239\n",
      "epoch: 4, train set index: 39, average loss: 0.9076395810\n",
      "epoch: 4, train set index: 59, average loss: 0.9178602308\n",
      "epoch: 4, train set index: 79, average loss: 0.9211989731\n",
      "epoch: 4, train set index: 99, average loss: 0.9156295031\n",
      "epoch: 4, train set index: 119, average loss: 0.8891785413\n",
      "epoch: 4, train set index: 139, average loss: 0.8514732897\n",
      "epoch: 4, train set index: 159, average loss: 0.8813319981\n",
      "epoch: 4, train set index: 179, average loss: 0.8832671523\n",
      "epoch: 4, train set index: 199, average loss: 0.9199372143\n",
      "epoch: 4, train set index: 219, average loss: 0.8950180799\n",
      "epoch: 4, train set index: 239, average loss: 0.8780298382\n",
      "epoch: 4, train set index: 259, average loss: 0.8190401852\n",
      "epoch: 4, train set index: 279, average loss: 0.8335433692\n",
      "epoch: 4, train set index: 299, average loss: 0.8986889899\n",
      "epoch: 4, train set index: 319, average loss: 0.8487513781\n",
      "epoch: 4, train set index: 339, average loss: 0.8393182188\n",
      "epoch: 4, train set index: 359, average loss: 0.8666961730\n",
      "epoch: 4, train set index: 379, average loss: 0.8437359363\n",
      "Epoch 4 finished, loss: 0.0068610097\n",
      "Starting testing\n",
      "Correct: 6791, total: 10000, accuracy: 0.6791000000, average loss: 0.0074167305\n",
      "Finished testing\n",
      "epoch: 5, train set index: 19, average loss: 0.8661790848\n",
      "epoch: 5, train set index: 39, average loss: 0.8620060503\n",
      "epoch: 5, train set index: 59, average loss: 0.8286041528\n",
      "epoch: 5, train set index: 79, average loss: 0.7857944667\n",
      "epoch: 5, train set index: 99, average loss: 0.7947000861\n",
      "epoch: 5, train set index: 119, average loss: 0.8478023767\n",
      "epoch: 5, train set index: 139, average loss: 0.7665202945\n",
      "epoch: 5, train set index: 159, average loss: 0.8244563609\n",
      "epoch: 5, train set index: 179, average loss: 0.8300666749\n",
      "epoch: 5, train set index: 199, average loss: 0.8142820686\n",
      "epoch: 5, train set index: 219, average loss: 0.8062280297\n",
      "epoch: 5, train set index: 239, average loss: 0.7843451440\n",
      "epoch: 5, train set index: 259, average loss: 0.8076559186\n",
      "epoch: 5, train set index: 279, average loss: 0.8044330746\n",
      "epoch: 5, train set index: 299, average loss: 0.7660023093\n",
      "epoch: 5, train set index: 319, average loss: 0.7946604043\n",
      "epoch: 5, train set index: 339, average loss: 0.8281802982\n",
      "epoch: 5, train set index: 359, average loss: 0.7634029061\n",
      "epoch: 5, train set index: 379, average loss: 0.7388607025\n",
      "Epoch 5 finished, loss: 0.0062963563\n",
      "Starting testing\n",
      "Correct: 7123, total: 10000, accuracy: 0.7123000000, average loss: 0.0066852862\n",
      "Finished testing\n",
      "epoch: 6, train set index: 19, average loss: 0.7498496741\n",
      "epoch: 6, train set index: 39, average loss: 0.7845747650\n",
      "epoch: 6, train set index: 59, average loss: 0.7335318565\n",
      "epoch: 6, train set index: 79, average loss: 0.7516725808\n",
      "epoch: 6, train set index: 99, average loss: 0.7717383444\n",
      "epoch: 6, train set index: 119, average loss: 0.7515022278\n",
      "epoch: 6, train set index: 139, average loss: 0.7975198865\n",
      "epoch: 6, train set index: 159, average loss: 0.7575101435\n",
      "epoch: 6, train set index: 179, average loss: 0.7307639331\n",
      "epoch: 6, train set index: 199, average loss: 0.7276475757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6, train set index: 219, average loss: 0.7487830669\n",
      "epoch: 6, train set index: 239, average loss: 0.7667578042\n",
      "epoch: 6, train set index: 259, average loss: 0.7252413273\n",
      "epoch: 6, train set index: 279, average loss: 0.7730973899\n",
      "epoch: 6, train set index: 299, average loss: 0.7316619903\n",
      "epoch: 6, train set index: 319, average loss: 0.7222951561\n",
      "epoch: 6, train set index: 339, average loss: 0.7194902033\n",
      "epoch: 6, train set index: 359, average loss: 0.7188881963\n",
      "epoch: 6, train set index: 379, average loss: 0.7399162471\n",
      "Epoch 6 finished, loss: 0.0058345427\n",
      "Starting testing\n",
      "Correct: 7121, total: 10000, accuracy: 0.7121000000, average loss: 0.0066183423\n",
      "Finished testing\n",
      "epoch: 7, train set index: 19, average loss: 0.7108396053\n",
      "epoch: 7, train set index: 39, average loss: 0.7189777792\n",
      "epoch: 7, train set index: 59, average loss: 0.7398421764\n",
      "epoch: 7, train set index: 79, average loss: 0.7636263967\n",
      "epoch: 7, train set index: 99, average loss: 0.7025730267\n",
      "epoch: 7, train set index: 119, average loss: 0.6913705289\n",
      "epoch: 7, train set index: 139, average loss: 0.6732420295\n",
      "epoch: 7, train set index: 159, average loss: 0.7022024691\n",
      "epoch: 7, train set index: 179, average loss: 0.7699104577\n",
      "epoch: 7, train set index: 199, average loss: 0.6630946666\n",
      "epoch: 7, train set index: 219, average loss: 0.7125574678\n",
      "epoch: 7, train set index: 239, average loss: 0.7582124859\n",
      "epoch: 7, train set index: 259, average loss: 0.6837266892\n",
      "epoch: 7, train set index: 279, average loss: 0.7263049364\n",
      "epoch: 7, train set index: 299, average loss: 0.7490560085\n",
      "epoch: 7, train set index: 319, average loss: 0.7244844884\n",
      "epoch: 7, train set index: 339, average loss: 0.6826122016\n",
      "epoch: 7, train set index: 359, average loss: 0.6879142225\n",
      "epoch: 7, train set index: 379, average loss: 0.6822901487\n",
      "Epoch 7 finished, loss: 0.0055709774\n",
      "Starting testing\n",
      "Correct: 7272, total: 10000, accuracy: 0.7272000000, average loss: 0.0062241501\n",
      "Finished testing\n",
      "epoch: 8, train set index: 19, average loss: 0.6387198806\n",
      "epoch: 8, train set index: 39, average loss: 0.6906672627\n",
      "epoch: 8, train set index: 59, average loss: 0.6743134499\n",
      "epoch: 8, train set index: 79, average loss: 0.6840589583\n",
      "epoch: 8, train set index: 99, average loss: 0.6618757069\n",
      "epoch: 8, train set index: 119, average loss: 0.6978335604\n",
      "epoch: 8, train set index: 139, average loss: 0.6838099658\n",
      "epoch: 8, train set index: 159, average loss: 0.6680297673\n",
      "epoch: 8, train set index: 179, average loss: 0.6446065977\n",
      "epoch: 8, train set index: 199, average loss: 0.6534917802\n",
      "epoch: 8, train set index: 219, average loss: 0.7009451658\n",
      "epoch: 8, train set index: 239, average loss: 0.6915536135\n",
      "epoch: 8, train set index: 259, average loss: 0.7224565297\n",
      "epoch: 8, train set index: 279, average loss: 0.6735578686\n",
      "epoch: 8, train set index: 299, average loss: 0.6581464961\n",
      "epoch: 8, train set index: 319, average loss: 0.6575682268\n",
      "epoch: 8, train set index: 339, average loss: 0.6419934466\n"
     ]
    }
   ],
   "source": [
    "write_file_and_close(global_output_filename, \"Start training\")\n",
    "\n",
    "it = 0\n",
    "for epoch in range(global_epoch_num):\n",
    "    if not check_control(global_control_filename):\n",
    "        write_file_and_close(gloabl_output_filename, \"Control llost\")\n",
    "    running_loss_sum = 0.\n",
    "    total_loss_sum = 0.\n",
    "    ctr_sum = 0\n",
    "    total_ctr = 0\n",
    "    for g in optimizer.param_groups:\n",
    "        g[\"lr\"] = lr_adjust(it)\n",
    "    for i, data in enumerate(trainloader):\n",
    "        info = [0., 0]\n",
    "        train(data, info)\n",
    "        running_loss_sum += info[0]\n",
    "        total_loss_sum += info[0]\n",
    "        ctr_sum += 1\n",
    "        total_ctr += info[1]\n",
    "        if (i + 1) % global_data_print_freq == 0:\n",
    "            write_file_and_close(global_output_filename,\n",
    "                \"epoch: {:d}, \"\n",
    "                \"train set index: {:d}, \"\n",
    "                \"average loss: {:.10f}\"\n",
    "                .format(epoch, i, running_loss_sum / ctr_sum)\n",
    "            )\n",
    "            running_loss_sum = 0.0\n",
    "            ctr_sum = 0\n",
    "        it = it + 1\n",
    "    write_file_and_close(global_output_filename,\n",
    "        \"Epoch {:d} finished, average loss: {:.10f}\"\n",
    "        .format(epoch, total_loss_sum / total_ctr)\n",
    "    )\n",
    "    if (epoch + 1) % global_epoch_test_freq == 0:\n",
    "        write_file_and_close(global_output_filename, \"Starting testing\")\n",
    "        info = [0., 0., 0.]\n",
    "        test(info)\n",
    "        write_file_and_close(global_output_filename,\n",
    "            \"Correct: {:d}, total: {:d}, \"\n",
    "            \"accuracy: {:.10f}, average loss: {:.10f}\"\n",
    "            .format(info[0], info[1], info[0] / info[1], info[2] / info[1])\n",
    "        )\n",
    "        write_file_and_close(global_output_filename, \"Finished testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
