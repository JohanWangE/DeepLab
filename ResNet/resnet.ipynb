{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "################### This is a line of 70 characters ##################\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils import write_file_and_close, check_control\n",
    "\n",
    "import os\n",
    "import errno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_batch_size = 128\n",
    "global_resnet_n = 3\n",
    "global_conv_bias = True\n",
    "global_data_print_freq = 20\n",
    "global_epoch_num = 200\n",
    "global_cuda_available = True\n",
    "global_output_filename = \"out.txt\"\n",
    "global_control_filename = \"control.txt\"\n",
    "global_epoch_test_freq = 1\n",
    "\n",
    "if global_cuda_available:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Here per-pixel mean isn't subtracted \n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", download=True, train=True, transform=transform_train\n",
    ")\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=global_batch_size, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", download=True, train=False, transform=transform_test\n",
    ")\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=global_batch_size, shuffle=False, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StartBlock(nn.Module):\n",
    "    \"\"\"First several blocks for resnet\n",
    "    \n",
    "    Only contains a single layer of conv2d and a batch norm layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, out_planes, kernel_size):\n",
    "        super(StartBlock, self).__init__()\n",
    "        self.out_plane = out_planes\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        self.conv = nn.Conv2d(\n",
    "            3, out_planes, kernel_size=kernel_size,\n",
    "            padding=1, bias=global_conv_bias\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_planes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = functional.relu(out)\n",
    "        return out\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"Repeated blockes for resnet\n",
    "    \n",
    "    Contains two conv layers, two batch norm layers and a shortcut\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.in_planes = in_planes\n",
    "        self.out_planes = out_planes\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, out_planes, kernel_size=kernel_size,\n",
    "            stride=stride, padding=1, bias=global_batch_size\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_planes, out_planes, kernel_size=kernel_size,\n",
    "            padding=1, bias=global_batch_size\n",
    "        )\n",
    "        self.shortcut = nn.Conv2d(\n",
    "            in_planes, out_planes, kernel_size=1, stride=stride\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.bn1(x)\n",
    "        out = functional.relu(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn2(out)\n",
    "        out = functional.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out += self.shortcut(x)\n",
    "        return out\n",
    "\n",
    "class EndBlock(nn.Module):\n",
    "    \"\"\"Last several blocks for resnet\n",
    "    \n",
    "    Only contains a global average pooling layer and a fully\n",
    "    connected layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_planes):\n",
    "        super(EndBlock, self).__init__()\n",
    "        self.fc = nn.Linear(in_planes, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.mean(x, dim=2)\n",
    "        out = torch.mean(out, dim=3)\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    \"\"\"ResNet-(6n + 2)\"\"\"\n",
    "    \n",
    "    def __init__(self, n):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.block_list = []\n",
    "        self.block_list.append(StartBlock(16, 3))\n",
    "        for i in range(n):\n",
    "            self.block_list.append(BasicBlock(16, 16, 3, 1))\n",
    "        self.block_list.append(BasicBlock(16, 32, 3, 2))\n",
    "        for i in range(n - 1):\n",
    "            self.block_list.append(BasicBlock(32, 32, 3, 1))\n",
    "        self.block_list.append(BasicBlock(32, 64, 3, 2))\n",
    "        for i in range(n - 1):\n",
    "            self.block_list.append(BasicBlock(64, 64, 3, 1))\n",
    "        self.block_list.append(EndBlock(64))\n",
    "        self.blocks = nn.Sequential(*self.block_list)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.blocks(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = ResNet(global_resnet_n)\n",
    "\n",
    "if global_cuda_available:\n",
    "    net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(\n",
    "    net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0001\n",
    ")\n",
    "\n",
    "def lr_adjust(it):\n",
    "    if it < 32000:\n",
    "        return 0.1\n",
    "    elif it < 48000:\n",
    "        return 0.01\n",
    "    elif it < 64000:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(data, info):\n",
    "    global net, optimizer, criterion\n",
    "    inputs, labels = data\n",
    "    inputs, labels = Variable(inputs), Variable(labels)\n",
    "    if global_cuda_available:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    info[0] = loss.data[0]\n",
    "    info[1] = labels.size()[0]\n",
    "\n",
    "def test(info):\n",
    "    global net\n",
    "    correct_sum = 0\n",
    "    total_loss_sum = 0.\n",
    "    total_ctr = 0\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        if global_cuda_available:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_ctr += labels.size()[0]\n",
    "        correct_sum += (predicted == labels.data).sum()\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss_sum += loss.data[0]\n",
    "    info[0] = correct_sum\n",
    "    info[1] = total_ctr\n",
    "    info[2] = total_loss_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file_and_close(global_output_filename, \"Cleaning...\", flag = \"w\")\n",
    "write_file_and_close(\n",
    "    global_output_filename,\n",
    "    \"The length of trainloader and testloader is {:d} and {:d} resp.\"\n",
    "    .format(len(trainloader), len(testloader))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "write_file_and_close(global_output_filename, \"Start training\")\n",
    "\n",
    "it = 0\n",
    "for epoch in range(global_epoch_num):\n",
    "    if not check_control(global_control_filename):\n",
    "        write_file_and_close(gloabl_output_filename, \"Control llost\")\n",
    "    running_loss_sum = 0.\n",
    "    total_loss_sum = 0.\n",
    "    ctr_sum = 0\n",
    "    total_ctr = 0\n",
    "    for g in optimizer.param_groups:\n",
    "        g[\"lr\"] = lr_adjust(it)\n",
    "    for i, data in enumerate(trainloader):\n",
    "        info = [0., 0]\n",
    "        train(data, info)\n",
    "        running_loss_sum += info[0]\n",
    "        total_loss_sum += info[0]\n",
    "        ctr_sum += 1\n",
    "        total_ctr += info[1]\n",
    "        if (i + 1) % global_data_print_freq == 0:\n",
    "            write_file_and_close(global_output_filename,\n",
    "                \"epoch: {:d}, \"\n",
    "                \"train set index: {:d}, \"\n",
    "                \"average loss: {:.10f}\"\n",
    "                .format(epoch, i, running_loss_sum / ctr_sum)\n",
    "            )\n",
    "            running_loss_sum = 0.0\n",
    "            ctr_sum = 0\n",
    "        it = it + 1\n",
    "    write_file_and_close(global_output_filename,\n",
    "        \"Epoch {:d} finished, average loss: {:.10f}\"\n",
    "        .format(epoch, total_loss_sum / total_ctr)\n",
    "    )\n",
    "    if (epoch + 1) % global_epoch_test_freq == 0:\n",
    "        write_file_and_close(global_output_filename, \"Starting testing\")\n",
    "        info = [0., 0., 0.]\n",
    "        test(info)\n",
    "        write_file_and_close(global_output_filename,\n",
    "            \"Correct: {:d}, total: {:d}, \"\n",
    "            \"accuracy: {:.10f}, average loss: {:.10f}\"\n",
    "            .format(info[0], info[1], info[0] / info[1], info[2] / info[1])\n",
    "        )\n",
    "        write_file_and_close(global_output_filename, \"Finished testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
