{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import init\n",
    "\n",
    "from utils import write_file_and_close, check_control\n",
    "from utils import generate_filename\n",
    "\n",
    "import os\n",
    "import errno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_batch_size = 128\n",
    "global_conv_bias = True\n",
    "global_data_print_freq = 20\n",
    "global_epoch_num = 200\n",
    "global_cuda_available = True\n",
    "global_output_filename = \"out.txt\"\n",
    "global_control_filename = \"control.txt\"\n",
    "global_epoch_test_freq = 1\n",
    "\n",
    "global_obselete_pytorch = False\n",
    "\n",
    "if global_cuda_available:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "global_weight_init_range = (-0.1, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_resnet_n = 5\n",
    "\n",
    "nn_arch_start = [16, 3]\n",
    "nn_arch_weight = (\n",
    "      [[16, 16, 3, 1]] * global_resnet_n\n",
    "    + [[16, 32, 3, 2]]\n",
    "    + [[32, 32, 3, 1]] * (global_resnet_n - 1)\n",
    "    + [[32, 64, 3, 2]]\n",
    "    + [[64, 64, 3, 1]] * (global_resnet_n - 1)\n",
    ")\n",
    "nn_arch_residue1 = (\n",
    "      [[16, 16, 1]] * global_resnet_n\n",
    "    + [[16, 32, 2]]\n",
    "    + [[32, 32, 1]] * (global_resnet_n - 1)\n",
    "    + [[32, 64, 2]]\n",
    "    + [[64, 64, 1]] * (global_resnet_n - 1)\n",
    ")\n",
    "nn_arch_residue2 = (\n",
    "      [None]\n",
    "    + [[16, 16, 1]] * (global_resnet_n - 1)\n",
    "    + [None] * 2\n",
    "    + [[32, 32, 1]] * (global_resnet_n - 2)\n",
    "    + [None] * 2\n",
    "    + [[64, 64, 1]] * (global_resnet_n - 2)\n",
    ")\n",
    "nn_arch_end = [64, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(28, padding=0),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# TODO: Calculate and subtract per-pixel average\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.CenterCrop(28),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", download=True, train=True, transform=transform_train\n",
    ")\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=global_batch_size, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", download=True, train=False, transform=transform_test\n",
    ")\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=global_batch_size, shuffle=False, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StartBlock(nn.Module):\n",
    "    \"\"\"First several blocks for resnet\n",
    "    \n",
    "    Only contains a single layer of conv2d and a batch norm layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, out_planes, kernel_size):\n",
    "        super(StartBlock, self).__init__()\n",
    "        self.out_plane = out_planes\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            3, out_planes, kernel_size=kernel_size,\n",
    "            padding=1, bias=global_conv_bias\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_planes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = functional.relu(out)\n",
    "        return out\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"Repeated blocks for resnet\n",
    "    \n",
    "    Contains two conv layers, two batch norm layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.in_planes = in_planes\n",
    "        self.out_planes = out_planes\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, out_planes, kernel_size=kernel_size,\n",
    "            stride=stride, padding=1, bias=global_conv_bias\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_planes, out_planes, kernel_size=kernel_size,\n",
    "            padding=1, bias=global_conv_bias\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.bn1(x)\n",
    "        out = functional.relu(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn2(out)\n",
    "        out = functional.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        return out\n",
    "\n",
    "class ShortcutBlock(nn.Module):\n",
    "    \"\"\"Shortcut blocks for resnet\n",
    "    \n",
    "    Contains a simple shortcut, which may be a simple identity or\n",
    "    a 1*1 conv layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_planes, out_planes, stride):\n",
    "        super(ShortcutBlock, self).__init__()\n",
    "        self.in_planes = in_planes\n",
    "        self.out_planes = out_planes\n",
    "        self.stride = stride\n",
    "        if in_planes != out_planes or stride != 1:\n",
    "            self.shortcut = nn.AvgPool2d(1, stride=stride, padding=0)\n",
    "        self.pad_size = []\n",
    "    \n",
    "    # Note: here check_pad is used to avoid frequent allocation of zeros,\n",
    "    # but the code here is rather ugly.\n",
    "    def check_pad(self, size):\n",
    "        if self.pad_size == size:\n",
    "            return self.pad\n",
    "        else:\n",
    "            if global_cuda_available:\n",
    "                pad = Variable(torch.zeros(size)).cuda()\n",
    "            else:\n",
    "                pad = Variable(torch.zeros(size))\n",
    "        self.pad = pad\n",
    "        self.pad_size = size\n",
    "        return self.pad\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.in_planes != self.out_planes or self.stride != 1:\n",
    "            out = self.shortcut(x)\n",
    "            size = list(out.size())\n",
    "            size[1] = self.out_planes - self.in_planes\n",
    "            out = torch.cat((out, self.check_pad(size)), 1)\n",
    "            return out\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "class EndBlock(nn.Module):\n",
    "    \"\"\"Last several blocks for resnet\n",
    "    \n",
    "    Only contains a global average pooling layer and a fully\n",
    "    connected layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_planes, out_classes):\n",
    "        super(EndBlock, self).__init__()\n",
    "        self.fc = nn.Linear(in_planes, out_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Note: In the new version of pytorch, let v be a tensor\n",
    "        # of size (3, 4, 5, 6), then the size of mean of axis 2 is \n",
    "        # (3, 4, 6) instead of (3, 4, 1, 6)\n",
    "        if global_obselete_pytorch:\n",
    "            out = torch.mean(x, dim=2)\n",
    "            out = torch.mean(out, dim=3)\n",
    "            out = out.view(out.size()[0], -1)\n",
    "        else:\n",
    "            out = torch.mean(x, dim=2)\n",
    "            out = torch.mean(out, dim=2)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WeightId(nn.Module):\n",
    "    \"\"\"A wrapped weight parameter\n",
    "    \n",
    "    Packed in order to be attached as attribute of an nn.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(WeightId, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(1))\n",
    "        self.weight.data.uniform_(*global_weight_init_range)\n",
    "    \n",
    "    def forward(self):\n",
    "        return self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ResNetWithWeight(nn.Module):\n",
    "    \"\"\"General ResNet with weight\n",
    "    \n",
    "    Contains BasicBlock, ShortcurBlock and momentum parameters.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, start, weight, residue1, residue2, end\n",
    "    ):\n",
    "        super(ResNetWithWeight, self).__init__()\n",
    "        self.start = start\n",
    "        self.weight = weight\n",
    "        self.residue1 = residue1\n",
    "        self.residue2 = residue2\n",
    "        self.end = end\n",
    "        self.start_block = StartBlock(*start)\n",
    "        self.weight_list = []\n",
    "        self.residue1_list = []\n",
    "        self.residue2_list = []\n",
    "        self.momentum_list = []\n",
    "        self.link_list = []\n",
    "        for w, r1, r2 in zip(weight, residue1, residue2):\n",
    "            rctr = 0\n",
    "            conv = BasicBlock(*w)\n",
    "            self.weight_list.append(conv)\n",
    "            if r1 is not None:\n",
    "                self.residue1_list.append(ShortcutBlock(*r1))\n",
    "                rctr += 1\n",
    "            else:\n",
    "                self.residue1_list.append(None)\n",
    "            if r2 is not None:\n",
    "                self.residue2_list.append(ShortcutBlock(*r2))\n",
    "                rctr += 2\n",
    "            else:\n",
    "                self.residue2_list.append(None)\n",
    "            self.link_list.append(rctr)\n",
    "            if rctr == 3:\n",
    "                self.momentum_list.append(WeightId())\n",
    "            else:\n",
    "                self.momentum_list.append(None)\n",
    "        self.end_block = EndBlock(*end)\n",
    "        self.weight_block = nn.Sequential(*filter(lambda u: u is not None, self.weight_list))\n",
    "        self.residue1_block = nn.Sequential(*filter(lambda u: u is not None, self.residue1_list))\n",
    "        self.residue2_block = nn.Sequential(*filter(lambda u: u is not None, self.residue2_list))\n",
    "        self.momentum_block = nn.Sequential(*filter(lambda u: u is not None, self.momentum_list))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.start_block(x)\n",
    "        out1 = None\n",
    "        out2 = None\n",
    "        for w, r1, r2, m, l in zip(\n",
    "            self.weight_list, self.residue1_list, self.residue2_list, self.momentum_list, self.link_list\n",
    "        ):\n",
    "            out2, out1 = out1, out\n",
    "            if l == 3:\n",
    "                if global_obselete_pytorch:\n",
    "                    tout2 = r2(out2)\n",
    "                    tout1 = r1(out1)\n",
    "                    out = m().expand_as(tout2) * tout2 + (1. - m()).expand_as(tout1) * tout1 + w(out1)\n",
    "                else:\n",
    "                    out = m() * r2(out2) + (1. - m()) * r1(out1) + w(out1)\n",
    "            elif l == 2:\n",
    "                out = r2(out2) + w(out1)\n",
    "            elif l == 1:\n",
    "                out = r1(out1) + w(out1)\n",
    "            elif l == 0:\n",
    "                out = w(out1)\n",
    "        out = self.end_block(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = ResNetWithWeight(nn_arch_start, nn_arch_weight, nn_arch_residue1, nn_arch_residue2, nn_arch_end)\n",
    "\n",
    "init.msra_init(net)\n",
    "\n",
    "if global_cuda_available:\n",
    "    net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(\n",
    "    net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0001\n",
    ")\n",
    "\n",
    "# TODO: Use scheduler to adjust learning rate\n",
    "def lr_adjust(it):\n",
    "    if it < 32000:\n",
    "        return 0.1\n",
    "    elif it < 48000:\n",
    "        return 0.01\n",
    "    elif it < 64000:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(data, info):\n",
    "    global net, optimizer, criterion\n",
    "    inputs, labels = data\n",
    "    inputs, labels = Variable(inputs), Variable(labels)\n",
    "    if global_cuda_available:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    info[0] = loss.data[0]\n",
    "    info[1] = labels.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(info):\n",
    "    global net\n",
    "    correct_sum = 0\n",
    "    total_loss_sum = 0.\n",
    "    total_ctr = 0\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        if global_cuda_available:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_ctr += labels.size()[0]\n",
    "        correct_sum += (predicted == labels.data).sum()\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss_sum += loss.data[0]\n",
    "    info[0] = correct_sum\n",
    "    info[1] = total_ctr\n",
    "    info[2] = total_loss_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file_and_close(global_output_filename, \"Cleaning...\", flag = \"w\")\n",
    "write_file_and_close(\n",
    "    global_output_filename,\n",
    "    \"The length of trainloader and testloader is {:d} and {:d} resp.\"\n",
    "    .format(len(trainloader), len(testloader))\n",
    ")\n",
    "\n",
    "write_file_and_close(global_output_filename, \"Start training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = 0\n",
    "for epoch in range(global_epoch_num):\n",
    "    if not check_control(global_control_filename):\n",
    "        write_file_and_close(gloabl_output_filename, \"Control lost\")\n",
    "    running_loss_sum = 0.\n",
    "    total_loss_sum = 0.\n",
    "    ctr_sum = 0\n",
    "    total_ctr = 0\n",
    "    for g in optimizer.param_groups:\n",
    "        g[\"lr\"] = lr_adjust(it)\n",
    "    for i, data in enumerate(trainloader):\n",
    "        info = [0., 0]\n",
    "        train(data, info)\n",
    "        running_loss_sum += info[0]\n",
    "        total_loss_sum += info[0]\n",
    "        ctr_sum += 1\n",
    "        total_ctr += info[1]\n",
    "        if (i + 1) % global_data_print_freq == 0:\n",
    "            write_file_and_close(global_output_filename,\n",
    "                \"epoch: {:d}, \"\n",
    "                \"train set index: {:d}, \"\n",
    "                \"average loss: {:.10f}\"\n",
    "                .format(epoch, i, running_loss_sum / ctr_sum)\n",
    "            )\n",
    "            running_loss_sum = 0.0\n",
    "            ctr_sum = 0\n",
    "        it = it + 1\n",
    "        if (i + 1) % global_data_print_freq == 0:\n",
    "            s = \"\"\n",
    "            for m in net.momentum_list:\n",
    "                if m is not None:\n",
    "                    s += \"{:.10f}, \".format(float(m.weight.data.cpu().numpy()))\n",
    "            write_file_and_close(global_output_filename,\n",
    "                \"momentum parameters: {:s}\".format(s[:-2])\n",
    "            )\n",
    "    write_file_and_close(global_output_filename,\n",
    "        \"Epoch {:d} finished, average loss: {:.10f}\"\n",
    "        .format(epoch, total_loss_sum / total_ctr)\n",
    "    )\n",
    "    if (epoch + 1) % global_epoch_test_freq == 0:\n",
    "        write_file_and_close(global_output_filename,\n",
    "                             \"Starting testing\"\n",
    "        )\n",
    "        info = [0., 0., 0.]\n",
    "        test(info)\n",
    "        write_file_and_close(global_output_filename,\n",
    "            \"Correct: {:d}, total: {:d}, \"\n",
    "            \"accuracy: {:.10f}, average loss: {:.10f}\"\n",
    "            .format(\n",
    "                info[0], info[1], info[0] / info[1], info[2] / info[1]\n",
    "            )\n",
    "        )\n",
    "        write_file_and_close(global_output_filename, \"Finished testing\")\n",
    "\n",
    "# TODO: Modify the filename\n",
    "model_filename = generate_filename()\n",
    "torch.save(net, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = numpy.zeros((3, 28, 28))\n",
    "for i, d in enumerate(trainloader):\n",
    "    d, _ = d\n",
    "    if i == 0:\n",
    "        print(d[2])\n",
    "    s += d.numpy().sum(axis=0)\n",
    "s /= 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
