{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from msra_init import msra_init\n",
    "\n",
    "from utils import write_file_and_close, check_control\n",
    "from utils import generate_filename\n",
    "\n",
    "import os\n",
    "import errno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_batch_size = 128\n",
    "global_resnet_n = 3\n",
    "global_conv_bias = False\n",
    "global_data_print_freq = 20\n",
    "global_epoch_num = 200\n",
    "global_cuda_available = True\n",
    "global_output_filename = \"out.txt\"\n",
    "global_control_filename = \"control.txt\"\n",
    "global_epoch_test_freq = 1\n",
    "\n",
    "if global_cuda_available:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "global_weight_init_range = (-0.1, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# TODO: Calculate and subtract per-pixel average\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", download=True, train=True, transform=transform_train\n",
    ")\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=global_batch_size, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", download=True, train=False, transform=transform_test\n",
    ")\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=global_batch_size, shuffle=False, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StartBlock(nn.Module):\n",
    "    \"\"\"First several blocks for resnet\n",
    "    \n",
    "    Only contains a single layer of conv2d and a batch norm layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, out_planes, kernel_size):\n",
    "        super(StartBlock, self).__init__()\n",
    "        self.out_plane = out_planes\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            3, out_planes, kernel_size=kernel_size,\n",
    "            padding=1, bias=global_conv_bias\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_planes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = functional.relu(out)\n",
    "        return out\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"Repeated blocks for resnet\n",
    "    \n",
    "    Contains two conv layers, two batch norm layers and a shortcut\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.in_planes = in_planes\n",
    "        self.out_planes = out_planes\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, out_planes, kernel_size=kernel_size,\n",
    "            stride=stride, padding=1, bias=global_conv_bias\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_planes, out_planes, kernel_size=kernel_size,\n",
    "            padding=1, bias=global_conv_bias\n",
    "        )\n",
    "        self.shortcut = nn.Conv2d(\n",
    "            in_planes, out_planes, kernel_size=1,\n",
    "            stride=stride, bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.bn1(x)\n",
    "        out = functional.relu(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn2(out)\n",
    "        out = functional.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        # TODO: Turn this criterion to a explicit argument\n",
    "        if self.stride != 1 or self.in_planes != self.out_planes:\n",
    "            out += self.shortcut(x)\n",
    "        return out\n",
    "\n",
    "class EndBlock(nn.Module):\n",
    "    \"\"\"Last several blocks for resnet\n",
    "    \n",
    "    Only contains a global average pooling layer and a fully\n",
    "    connected layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_planes, out_classes):\n",
    "        super(EndBlock, self).__init__()\n",
    "        self.fc = nn.Linear(in_planes, out_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.mean(x, dim=2)\n",
    "        # Note: In the new version of pytorch, let v be a tensor\n",
    "        # of size (3, 4, 5, 6), then the size of mean of axis 2 is \n",
    "        # (3, 4, 6) instead of (3, 4, 1, 6)\n",
    "        out = torch.mean(out, dim=2)\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WeightId(nn.Module):\n",
    "    \"\"\"A wrapped weight parameter\n",
    "    \n",
    "    Packed in order to be attached as attribute of an nn.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(WeightId, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(1))\n",
    "        self.weight.data.uniform_(*global_weight_init_range)\n",
    "    \n",
    "    def forward(self):\n",
    "        return self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NMResidualBlock(nn.Module):\n",
    "    \"\"\"Nesterov momentum residual block for resnet\n",
    "    \n",
    "    Contains several BasicBlock and a momentum parameter.\n",
    "    \"\"\"\n",
    "    # Note: expansion here is a dummy parameter, should be equal\n",
    "    # to out_planes / in_planes\n",
    "    def __init__(\n",
    "        self, in_planes, out_planes,\n",
    "        kernel_size, blocks, expansion = 2\n",
    "    ):\n",
    "        super(NMResidualBlock, self).__init__()\n",
    "        self.blocks = blocks\n",
    "        self.expansion = expansion\n",
    "        self.basic_blocks = []\n",
    "        self.basic_blocks.append(\n",
    "            BasicBlock(in_planes, out_planes, kernel_size, expansion)\n",
    "        )\n",
    "        for i in range(blocks - 1):\n",
    "            self.basic_blocks.append(\n",
    "                BasicBlock(out_planes, out_planes, kernel_size, 1)\n",
    "            )\n",
    "        # Note: add basic_block into Net's parameter!!!\n",
    "        self.para_blocks = nn.Sequential(*self.basic_blocks)\n",
    "        self.momentum = []\n",
    "        for i in range(blocks - 1):\n",
    "            self.momentum.append(WeightId())\n",
    "        self.para_momentum = nn.Sequential(*self.momentum)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Rewrite the fp function with iterables\n",
    "        if self.expansion == 1:\n",
    "            oldout = x\n",
    "            out = self.basic_blocks[0](x)\n",
    "            start = 1\n",
    "        else:\n",
    "            oldout = self.basic_blocks[0](x)\n",
    "            out = self.basic_blocks[1](oldout)\n",
    "            start = 2\n",
    "        for i in range(start, self.blocks):\n",
    "            newout = self.basic_blocks[i](out)\n",
    "            newout += (\n",
    "                  self.momentum[i-1]() * oldout\n",
    "                + (1. - self.momentum[i-1]()) * out\n",
    "            )\n",
    "            oldout = out\n",
    "            out = newout\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NMResNet(nn.Module):\n",
    "    \"\"\"ResNet-(6n + 2) with Nesterov momentum for CIFAR-10 dataset\"\"\"\n",
    "    def __init__(self, block_num, out_classes=10):\n",
    "        super(NMResNet,self).__init__()\n",
    "        self.block_list = []\n",
    "        self.block_list.append(StartBlock(16, 3))\n",
    "        self.block_list.append(\n",
    "            NMResidualBlock(16, 16, 3, block_num, expansion=1)\n",
    "        )\n",
    "        self.block_list.append(NMResidualBlock(16, 32, 3, block_num))\n",
    "        self.block_list.append(NMResidualBlock(32, 64, 3, block_num))\n",
    "        self.block_list.append(EndBlock(64, out_classes=out_classes))\n",
    "        self.blocks = nn.Sequential(*self.block_list)\n",
    "        msra_init(self)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.blocks(x)\n",
    "        return out\n",
    "\n",
    "net = NMResNet(global_resnet_n)\n",
    "\n",
    "if global_cuda_available:\n",
    "    net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(\n",
    "    net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0001\n",
    ")\n",
    "\n",
    "# TODO: Use scheduler to adjust learning rate\n",
    "def lr_adjust(it):\n",
    "    if it < 32000:\n",
    "        return 0.1\n",
    "    elif it < 48000:\n",
    "        return 0.01\n",
    "    elif it < 64000:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(data, info):\n",
    "    global net, optimizer, criterion\n",
    "    inputs, labels = data\n",
    "    inputs, labels = Variable(inputs), Variable(labels)\n",
    "    if global_cuda_available:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    info[0] = loss.data[0]\n",
    "    info[1] = labels.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(info):\n",
    "    global net\n",
    "    correct_sum = 0\n",
    "    total_loss_sum = 0.\n",
    "    total_ctr = 0\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        if global_cuda_available:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_ctr += labels.size()[0]\n",
    "        correct_sum += (predicted == labels.data).sum()\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss_sum += loss.data[0]\n",
    "    info[0] = correct_sum\n",
    "    info[1] = total_ctr\n",
    "    info[2] = total_loss_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file_and_close(global_output_filename, \"Cleaning...\", flag = \"w\")\n",
    "write_file_and_close(\n",
    "    global_output_filename,\n",
    "    \"The length of trainloader and testloader is {:d} and {:d} resp.\"\n",
    "    .format(len(trainloader), len(testloader))\n",
    ")\n",
    "\n",
    "write_file_and_close(global_output_filename, \"Start training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = 0\n",
    "for epoch in range(global_epoch_num):\n",
    "    if not check_control(global_control_filename):\n",
    "        write_file_and_close(gloabl_output_filename, \"Control lost\")\n",
    "    running_loss_sum = 0.\n",
    "    total_loss_sum = 0.\n",
    "    ctr_sum = 0\n",
    "    total_ctr = 0\n",
    "    for g in optimizer.param_groups:\n",
    "        g[\"lr\"] = lr_adjust(it)\n",
    "    for i, data in enumerate(trainloader):\n",
    "        info = [0., 0]\n",
    "        train(data, info)\n",
    "        running_loss_sum += info[0]\n",
    "        total_loss_sum += info[0]\n",
    "        ctr_sum += 1\n",
    "        total_ctr += info[1]\n",
    "        if (i + 1) % global_data_print_freq == 0:\n",
    "            write_file_and_close(global_output_filename,\n",
    "                \"epoch: {:d}, \"\n",
    "                \"train set index: {:d}, \"\n",
    "                \"average loss: {:.10f}\"\n",
    "                .format(epoch, i, running_loss_sum / ctr_sum)\n",
    "            )\n",
    "            running_loss_sum = 0.0\n",
    "            ctr_sum = 0\n",
    "        it = it + 1\n",
    "    write_file_and_close(global_output_filename,\n",
    "        \"Epoch {:d} finished, average loss: {:.10f}\"\n",
    "        .format(epoch, total_loss_sum / total_ctr)\n",
    "    )\n",
    "    if (epoch + 1) % global_epoch_test_freq == 0:\n",
    "        write_file_and_close(global_output_filename,\n",
    "                             \"Starting testing\"\n",
    "        )\n",
    "        info = [0., 0., 0.]\n",
    "        test(info)\n",
    "        write_file_and_close(global_output_filename,\n",
    "            \"Correct: {:d}, total: {:d}, \"\n",
    "            \"accuracy: {:.10f}, average loss: {:.10f}\"\n",
    "            .format(\n",
    "                info[0], info[1], info[0] / info[1], info[2] / info[1]\n",
    "            )\n",
    "        )\n",
    "        write_file_and_close(global_output_filename, \"Finished testing\")\n",
    "\n",
    "# TODO: Modify the filename\n",
    "model_filename = generate_filename()\n",
    "torch.save(net, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
